#!/usr/bin/env python3
"""
åå·¥æ•™åŠ¡å¤„çˆ¬è™« - AI å¢å¼ºç‰ˆ
é›†æˆç¡…åŸºæµåŠ¨ API è‡ªåŠ¨ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
"""

import sys
import os

# æ·»åŠ è„šæœ¬ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from fetch_scut_jw import *
from ai_summarizer import generate_summary

# ç¡…åŸºæµåŠ¨ API Key
SILICONFLOW_API_KEY = "sk-jglsdgaygjgjcgttnqxcadbpfgpncphvkggkwvixsaclbkfa"


def process_notices_with_ai(notices: List[Dict], limit: int = 10) -> List[Dict]:
    """
    å¤„ç†é€šçŸ¥åˆ—è¡¨ï¼ŒæŠ“å–è¯¦æƒ…å¹¶ç”Ÿæˆ AI æ‘˜è¦

    Args:
        notices: é€šçŸ¥åˆ—è¡¨
        limit: æœ€å¤šå¤„ç†çš„é€šçŸ¥æ•°é‡

    Returns:
        åŒ…å« AI æ‘˜è¦çš„æ–‡ç« åˆ—è¡¨
    """
    articles = []

    print(f"\nå¼€å§‹å¤„ç†é€šçŸ¥è¯¦æƒ…ï¼ˆé™åˆ¶ {limit} æ¡ï¼Œå¯ç”¨ AI æ‘˜è¦ï¼‰...", file=sys.stderr)

    for i, notice in enumerate(notices[:limit], 1):
        print(f"[{i}/{min(limit, len(notices))}] å¤„ç†: {notice['title'][:30]}...", file=sys.stderr)

        # æŠ“å–è¯¦æƒ…é¡µ
        content, publish_date = fetch_notice_detail(notice['url'])

        if not content:
            content = "**å†…å®¹æŠ“å–å¤±è´¥ï¼Œè¯·è®¿é—®åŸæ–‡é“¾æ¥æŸ¥çœ‹è¯¦æƒ…ã€‚**"
            publish_date = notice['date']

        final_date = publish_date if publish_date else notice['date']

        # è®¡ç®—ä¼˜å…ˆçº§å’Œæ ‡ç­¾
        priority = calculate_priority(notice['title'], content)
        tags = extract_tags(notice['title'], content)

        if 'category' in notice and notice['category']:
            if notice['category'] not in tags:
                tags.insert(0, notice['category'])

        # ç”Ÿæˆç®€çŸ­æ‘˜è¦
        content_text = content.replace('#', '').replace('*', '').replace('>', '').strip()
        summary = content_text[:200] + '...' if len(content_text) > 200 else content_text

        # ğŸ¤– è°ƒç”¨ AI ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
        print(f"    æ­£åœ¨ç”Ÿæˆ AI æ‘˜è¦...", file=sys.stderr)
        ai_summary = generate_summary(
            content=content,
            content_type='notice',
            api_key=SILICONFLOW_API_KEY
        )

        # æ„é€  Markdown æ ¼å¼æ­£æ–‡ï¼ˆåŒ…å« AI æ‘˜è¦ï¼‰
        priority_emoji = 'ğŸ”´' if priority == 'high' else 'ğŸ”µ'

        # å¦‚æœæœ‰ AI æ‘˜è¦ï¼Œæ’å…¥åˆ°æ­£æ–‡å¼€å¤´
        ai_section = f"\n\n{ai_summary}\n\n---\n" if ai_summary else ""

        full_content = f"""# {notice['title']}

> ğŸ“… å‘å¸ƒæ—¥æœŸ: {final_date}
> ğŸ·ï¸ åˆ†ç±»: {notice.get('category', 'é€šçŸ¥')}
> {priority_emoji} ä¼˜å…ˆçº§: **{priority.upper()}**
> ğŸ”— åŸæ–‡é“¾æ¥: [{notice['url']}]({notice['url']})

---
{ai_section}
{content}

---

*æœ¬æ–‡ç”± Anthropo-Reader è‡ªåŠ¨æŠ“å–æ•´ç† | æ•°æ®æ¥æº: åå—ç†å·¥å¤§å­¦æœ¬ç§‘ç”Ÿé™¢ | AI æ‘˜è¦ç”±ç¡…åŸºæµåŠ¨æä¾›æ”¯æŒ*
"""

        # æ„é€ æ•°æ®åº“è®°å½•
        article = {
            'title': notice['title'],
            'summary': summary,
            'content': full_content,
            'source': 'SCUT_JW',
            'source_url': notice['url'],
            'author': 'åå—ç†å·¥å¤§å­¦æœ¬ç§‘ç”Ÿé™¢',
            'published_at': final_date,
            'fetched_at': datetime.now().isoformat(),
            'priority': priority,
            'tags': tags[:5],
            'is_favorited': False
        }

        articles.append(article)

        # ç¤¼è²Œå»¶è¿Ÿ
        time.sleep(random.uniform(2, 4))

    print(f"\nå¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(articles)} æ¡ç»“æ„åŒ–æ•°æ®ï¼ˆå« AI æ‘˜è¦ï¼‰", file=sys.stderr)
    return articles


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åå·¥æ•™åŠ¡å¤„çˆ¬è™«ï¼ˆAI å¢å¼ºç‰ˆï¼‰')
    parser.add_argument('--pages', type=int, default=2, help='æŠ“å–é¡µæ•°ï¼ˆé»˜è®¤ 2ï¼‰')
    parser.add_argument('--limit', type=int, default=5, help='å¤„ç†é€šçŸ¥æ•°é‡ï¼ˆé»˜è®¤ 5ï¼Œå»ºè®®ä¸è¶…è¿‡ 10 ä»¥æ§åˆ¶ API æˆæœ¬ï¼‰')
    parser.add_argument('--category', type=int, default=0, help='é€šçŸ¥åˆ†ç±»')
    parser.add_argument('--output', default='', help='è¾“å‡º JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--upload', action='store_true', help='ä¸Šä¼ åˆ° Supabase')
    parser.add_argument('--table', default='school_notices', help='Supabase è¡¨å')
    parser.add_argument('--no-ai', action='store_true', help='ç¦ç”¨ AI æ‘˜è¦ï¼ˆèŠ‚çœ API è°ƒç”¨ï¼‰')

    default_url = "https://ovytvktzhuapvictznnr.supabase.co"
    default_key = "sb_secret_3DFQXsH8RqIldbdJAVrC5Q_A_xtu9uh"

    parser.add_argument('--supabase-url', default=default_url)
    parser.add_argument('--supabase-key', default=default_key)

    args = parser.parse_args()

    # Step 1: æŠ“å–é€šçŸ¥åˆ—è¡¨
    notices = fetch_notice_list(max_pages=args.pages, category=args.category)

    if not notices:
        print("âš ï¸  æœªæŠ“å–åˆ°ä»»ä½•é€šçŸ¥", file=sys.stderr)
        sys.exit(1)

    print(f"\nâœ… å…±æŠ“å–åˆ° {len(notices)} æ¡é€šçŸ¥", file=sys.stderr)

    # Step 2: å¤„ç†é€šçŸ¥è¯¦æƒ…ï¼ˆé€‰æ‹©æ˜¯å¦ä½¿ç”¨ AIï¼‰
    if args.no_ai:
        articles = process_notices(notices, limit=args.limit)
    else:
        articles = process_notices_with_ai(notices, limit=args.limit)

    # Step 3: è¾“å‡ºåˆ°æ–‡ä»¶
    if args.output:
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(articles, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {args.output}", file=sys.stderr)
        except IOError as e:
            print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}", file=sys.stderr)
    elif not args.upload:
        print(json.dumps(articles, indent=2, ensure_ascii=False))

    # Step 4: ä¸Šä¼ åˆ° Supabase
    if args.upload:
        url = args.supabase_url or os.environ.get('SUPABASE_URL')
        key = args.supabase_key or os.environ.get('SUPABASE_KEY')

        if url and key:
            save_to_supabase(articles, url, key, args.table)
        else:
            print("âŒ é”™è¯¯: éœ€è¦æä¾› Supabase URL å’Œ Key", file=sys.stderr)


if __name__ == '__main__':
    main()
