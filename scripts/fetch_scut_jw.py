#!/usr/bin/env python3
"""
SCUT JW (åå—ç†å·¥å¤§å­¦æ•™åŠ¡å¤„) Crawler
Fetches academic notices from SCUT JW website and uploads to Supabase
é’ˆå¯¹å¾®ç”µå­ä¸“ä¸šçš„æ™ºèƒ½è¿‡æ»¤çˆ¬è™«
"""

import requests
from bs4 import BeautifulSoup
import html2text
import json
import os
import sys
import argparse
import time
import random
from datetime import datetime
from typing import List, Dict, Optional

# Try to import supabase
try:
    from supabase import create_client, Client
except ImportError:
    create_client = None


# ==================== é…ç½®åŒº ====================

# å…³é”®è¯ä¼˜å…ˆçº§é…ç½®
HIGH_PRIORITY_KEYWORDS = [
    "å¾®ç”µå­", "é›†æˆç”µè·¯", "èŠ¯ç‰‡", "åŠå¯¼ä½“",
    "ä¿ç ”", "æ¨å…", "å®ä¹ ",
    "å¹¿å·å›½é™…æ ¡åŒº", "GZIC", "å›½é™…æ ¡åŒº",
    "ç”µå­ç§‘å­¦ä¸æŠ€æœ¯", "å¾®ç”µå­ç§‘å­¦ä¸å·¥ç¨‹"
]

LOW_PRIORITY_KEYWORDS = [
    "é€‰è¯¾", "æ”¾å‡", "é€šçŸ¥", "è€ƒè¯•", "è¡¥è€ƒ", "é‡ä¿®",
    "æ•™å­¦", "è¯¾ç¨‹", "æˆç»©", "å­¦åˆ†"
]

# User-Agent æ± ï¼ˆåçˆ¬è™«ï¼‰
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
]

# æ•™åŠ¡å¤„ç½‘ç«™é…ç½®
JW_BASE_URL = "https://jw.scut.edu.cn"
JW_NOTICE_URL = f"{JW_BASE_URL}/zhinan/cms/toPosts.do"
JW_API_URL = f"{JW_BASE_URL}/zhinan/cms/article/v2/findInformNotice.do"  # AJAX API æ¥å£


# ==================== æ ¸å¿ƒåŠŸèƒ½ ====================

def get_random_headers() -> Dict:
    """ç”Ÿæˆéšæœº HTTP è¯·æ±‚å¤´ï¼ˆåçˆ¬è™«ï¼‰"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Connection': 'keep-alive',
    }


def calculate_priority(title: str, content: str) -> str:
    """
    åŸºäºå…³é”®è¯è®¡ç®—é€šçŸ¥ä¼˜å…ˆçº§
    è¿”å›: 'high' | 'low'
    """
    text = (title + " " + content).lower()

    # é«˜ä¼˜å…ˆçº§å…³é”®è¯åŒ¹é…
    for keyword in HIGH_PRIORITY_KEYWORDS:
        if keyword.lower() in text:
            return 'high'

    # ä½ä¼˜å…ˆçº§å…³é”®è¯åŒ¹é…
    for keyword in LOW_PRIORITY_KEYWORDS:
        if keyword.lower() in text:
            return 'low'

    # é»˜è®¤ä½ä¼˜å…ˆçº§
    return 'low'


def extract_tags(title: str, content: str) -> List[str]:
    """æå–æ–‡ç« æ ‡ç­¾"""
    tags = set()
    text = (title + " " + content).lower()

    # åˆå¹¶æ‰€æœ‰å…³é”®è¯
    all_keywords = HIGH_PRIORITY_KEYWORDS + LOW_PRIORITY_KEYWORDS
    for keyword in all_keywords:
        if keyword.lower() in text:
            tags.add(keyword)

    return list(tags)[:5]  # æœ€å¤šè¿”å› 5 ä¸ªæ ‡ç­¾


def fetch_notice_list(max_pages: int = 3, category: int = 0) -> List[Dict]:
    """
    æŠ“å–æ•™åŠ¡å¤„é€šçŸ¥åˆ—è¡¨ï¼ˆé€šè¿‡ AJAX APIï¼‰

    Args:
        max_pages: æœ€å¤§æŠ“å–é¡µæ•°
        category: é€šçŸ¥åˆ†ç±» (0=å…¨éƒ¨, 1=é€‰è¯¾, 2=è€ƒè¯•, 3=å®è·µ, 4=äº¤æµ, 5=æ•™å¸ˆ, 6=ä¿¡æ¯)

    Returns:
        é€šçŸ¥åˆ—è¡¨ [{'title': str, 'url': str, 'date': str, 'category': str}, ...]
    """
    notices = []

    print(f"å¼€å§‹é€šè¿‡ API æŠ“å–æ•™åŠ¡å¤„é€šçŸ¥ï¼ˆç±»åˆ«: {category}, æœ€å¤š {max_pages} é¡µï¼‰...", file=sys.stderr)

    # åˆ›å»º Session å¯¹è±¡ï¼ˆé‡è¦ï¼šéœ€è¦å…ˆè®¿é—®ä¸»é¡µè·å– Cookieï¼‰
    session = requests.Session()

    try:
        # Step 1: è®¿é—®ä¸»é¡µè·å– JSESSIONID
        print("æ­£åœ¨è·å– Session Cookie...", file=sys.stderr)
        session.get(JW_NOTICE_URL, headers=get_random_headers(), timeout=10)
    except Exception as e:
        print(f"è·å– Session å¤±è´¥: {e}", file=sys.stderr)

    for page in range(1, max_pages + 1):
        try:
            # æ„é€  API è¯·æ±‚å‚æ•°
            payload = {
                'category': str(category),
                'tag': str(category),
                'pageNum': page,
                'pageSize': 15,
                'keyword': ''
            }

            # æ„é€ å®Œæ•´çš„è¯·æ±‚å¤´ï¼ˆåŒ…å« AJAX æ ‡è¯†ï¼‰
            headers = get_random_headers()
            headers.update({
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
                'X-Requested-With': 'XMLHttpRequest',
                'Referer': JW_NOTICE_URL,
                'Origin': JW_BASE_URL
            })

            # POST è¯·æ±‚åˆ° AJAX APIï¼ˆä½¿ç”¨ sessionï¼‰
            response = session.post(
                JW_API_URL,
                data=payload,
                headers=headers,
                timeout=15
            )
            response.raise_for_status()

            # è§£æ JSON å“åº”
            data = response.json()

            if not data.get('success', False):
                print(f"API è¿”å›é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}", file=sys.stderr)
                break

            if 'list' not in data or not data['list']:
                print(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œåœæ­¢æŠ“å–", file=sys.stderr)
                break

            page_count = 0
            for item in data['list']:
                try:
                    # æå–å­—æ®µï¼ˆä½¿ç”¨å®é™…çš„ API å­—æ®µåï¼‰
                    article_id = item.get('id', '')
                    title = item.get('title', '').strip()  # âœ… ä¿®æ­£ï¼šä½¿ç”¨ 'title' è€Œé 'postTitle'
                    create_time = item.get('createTime', datetime.now().strftime('%Y.%m.%d'))
                    tag = item.get('tag', 0)  # âœ… ä¿®æ­£ï¼šä½¿ç”¨ 'tag' å­—æ®µ

                    # æ ¼å¼åŒ–æ—¥æœŸï¼ˆä» 2026.01.16 è½¬ä¸º 2026-01-16ï¼‰
                    if '.' in create_time:
                        parts = create_time.split('.')
                        if len(parts) == 3:
                            create_time = f"20{parts[0]}-{parts[1]}-{parts[2]}"

                    # æ„é€ è¯¦æƒ…é¡µ URL
                    url = f"{JW_BASE_URL}/zhinan/cms/article/view.do?type=posts&id={article_id}"

                    # åˆ†ç±»æ˜ å°„ï¼ˆæ ¹æ® tag å­—æ®µï¼‰
                    category_map = {
                        1: 'é€‰è¯¾',
                        2: 'è€ƒè¯•',
                        3: 'å®è·µ',
                        4: 'äº¤æµ',
                        5: 'æ•™å¸ˆ',
                        6: 'ä¿¡æ¯'
                    }
                    category_name = category_map.get(tag, 'é€šçŸ¥')

                    # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                    if title and article_id:
                        notices.append({
                            'title': title,
                            'url': url,
                            'date': create_time,
                            'category': category_name,
                            'id': article_id
                        })
                        page_count += 1

                except Exception as e:
                    print(f"è§£æå•æ¡é€šçŸ¥æ—¶å‡ºé”™: {e}", file=sys.stderr)
                    continue

            print(f"ç¬¬ {page} é¡µæŠ“å–å®Œæˆï¼Œæœ¬é¡µ {page_count} æ¡ï¼Œç´¯è®¡ {len(notices)} æ¡", file=sys.stderr)

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
            total = data.get('total', 0)
            if len(notices) >= total:
                print(f"å·²æŠ“å–å…¨éƒ¨é€šçŸ¥ï¼ˆå…± {total} æ¡ï¼‰ï¼Œåœæ­¢", file=sys.stderr)
                break

            # ç¤¼è²Œå»¶è¿Ÿ
            time.sleep(random.uniform(1.5, 3))

        except requests.exceptions.RequestException as e:
            print(f"API è¯·æ±‚å¤±è´¥ï¼ˆç¬¬ {page} é¡µï¼‰: {e}", file=sys.stderr)
            break
        except json.JSONDecodeError as e:
            print(f"JSON è§£æå¤±è´¥ï¼ˆç¬¬ {page} é¡µï¼‰: {e}", file=sys.stderr)
            break

    # å»é‡
    unique_notices = {n['id']: n for n in notices}.values()
    final_list = list(unique_notices)
    print(f"å»é‡åå…± {len(final_list)} æ¡å”¯ä¸€é€šçŸ¥", file=sys.stderr)
    return final_list


def fetch_notice_detail(notice_url: str) -> tuple[Optional[str], Optional[str]]:
    """
    æŠ“å–é€šçŸ¥è¯¦æƒ…é¡µå†…å®¹

    Args:
        notice_url: é€šçŸ¥è¯¦æƒ…é¡µ URL

    Returns:
        (Markdown æ ¼å¼çš„æ­£æ–‡å†…å®¹, å‘å¸ƒæ—¥æœŸ)
    """
    try:
        response = requests.get(
            notice_url,
            headers=get_random_headers(),
            timeout=15,
            verify=True
        )
        response.raise_for_status()
        response.encoding = 'utf-8'

        soup = BeautifulSoup(response.text, 'html.parser')

        # æå–å‘å¸ƒæ—¥æœŸï¼ˆå¤šç§å¯èƒ½çš„ä½ç½®ï¼‰
        publish_date = None
        date_patterns = [
            soup.find('span', class_='publish-date'),
            soup.find('div', class_='post-date'),
            soup.find('time'),
        ]

        for date_elem in date_patterns:
            if date_elem:
                publish_date = date_elem.get_text(strip=True)
                break

        if not publish_date:
            publish_date = datetime.now().strftime('%Y-%m-%d')

        # æŸ¥æ‰¾æ­£æ–‡å†…å®¹ï¼ˆæ ¹æ®å®é™…ç½‘ç«™ç»“æ„ï¼‰
        content_div = (
            soup.find('div', class_='article-content') or
            soup.find('div', class_='post-content') or
            soup.find('div', class_='content') or
            soup.find('div', id='content') or
            soup.find('article')
        )

        if not content_div:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæå–ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = soup.find('main') or soup.find('div', class_='main')
            if main_content:
                # ç§»é™¤å¯¼èˆªã€ä¾§è¾¹æ ç­‰å¹²æ‰°å…ƒç´ 
                for unwanted in main_content.find_all(['nav', 'aside', 'header', 'footer']):
                    unwanted.decompose()
                content_html = str(main_content)
            else:
                # æœ€åå¤‡ç”¨ï¼šæå–æ‰€æœ‰æ®µè½
                paragraphs = soup.find_all('p')
                content_html = ''.join(str(p) for p in paragraphs)
        else:
            content_html = str(content_div)

        # HTML è½¬ Markdown
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = False
        h.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
        h.unicode_snob = True  # ä¿æŒ Unicode å­—ç¬¦

        markdown_content = h.handle(content_html)

        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        markdown_content = '\n'.join(
            line for line in markdown_content.split('\n')
            if line.strip() or line == ''
        )

        return markdown_content.strip(), publish_date

    except Exception as e:
        print(f"æŠ“å–è¯¦æƒ…é¡µå¤±è´¥ ({notice_url}): {e}", file=sys.stderr)
        return None, None


def process_notices(notices: List[Dict], limit: int = 10) -> List[Dict]:
    """
    å¤„ç†é€šçŸ¥åˆ—è¡¨ï¼ŒæŠ“å–è¯¦æƒ…å¹¶ç”Ÿæˆç»“æ„åŒ–æ•°æ®

    Args:
        notices: é€šçŸ¥åˆ—è¡¨
        limit: æœ€å¤šå¤„ç†æ¡æ•°

    Returns:
        ç»“æ„åŒ–æ–‡ç« æ•°æ®
    """
    articles = []

    print(f"\nå¼€å§‹å¤„ç†é€šçŸ¥è¯¦æƒ…ï¼ˆé™åˆ¶ {limit} æ¡ï¼‰...", file=sys.stderr)

    for i, notice in enumerate(notices[:limit], 1):
        print(f"[{i}/{min(limit, len(notices))}] å¤„ç†: {notice['title'][:30]}...", file=sys.stderr)

        # æŠ“å–è¯¦æƒ…é¡µ
        content, publish_date = fetch_notice_detail(notice['url'])

        if not content:
            content = "**å†…å®¹æŠ“å–å¤±è´¥ï¼Œè¯·è®¿é—®åŸæ–‡é“¾æ¥æŸ¥çœ‹è¯¦æƒ…ã€‚**"
            publish_date = notice['date']

        # ä½¿ç”¨è¯¦æƒ…é¡µçš„æ—¥æœŸï¼ˆå¦‚æœæœ‰ï¼‰
        final_date = publish_date if publish_date else notice['date']

        # è®¡ç®—ä¼˜å…ˆçº§å’Œæ ‡ç­¾
        priority = calculate_priority(notice['title'], content)
        tags = extract_tags(notice['title'], content)

        # æ·»åŠ åˆ†ç±»æ ‡ç­¾
        if 'category' in notice and notice['category']:
            if notice['category'] not in tags:
                tags.insert(0, notice['category'])

        # ç”Ÿæˆç®€çŸ­æ‘˜è¦ï¼ˆå–å†…å®¹å‰200å­—ç¬¦ï¼Œå»é™¤ Markdown ç¬¦å·ï¼‰
        content_text = content.replace('#', '').replace('*', '').replace('>', '').strip()
        summary = content_text[:200] + '...' if len(content_text) > 200 else content_text

        # æ„é€  Markdown æ ¼å¼æ­£æ–‡
        priority_emoji = 'ğŸ”´' if priority == 'high' else 'ğŸ”µ'
        full_content = f"""# {notice['title']}

> ğŸ“… å‘å¸ƒæ—¥æœŸ: {final_date}
> ğŸ·ï¸ åˆ†ç±»: {notice.get('category', 'é€šçŸ¥')}
> {priority_emoji} ä¼˜å…ˆçº§: **{priority.upper()}**
> ğŸ”— åŸæ–‡é“¾æ¥: [{notice['url']}]({notice['url']})

---

{content}

---

*æœ¬æ–‡ç”± Anthropo-Reader è‡ªåŠ¨æŠ“å–æ•´ç† | æ•°æ®æ¥æº: åå—ç†å·¥å¤§å­¦æœ¬ç§‘ç”Ÿé™¢*
"""

        # æ„é€ æ•°æ®åº“è®°å½•
        article = {
            'title': notice['title'],
            'summary': summary,
            'content': full_content,
            'source': 'SCUT_JW',
            'source_url': notice['url'],
            'author': 'åå—ç†å·¥å¤§å­¦æœ¬ç§‘ç”Ÿé™¢',
            'published_at': final_date,
            'fetched_at': datetime.now().isoformat(),
            'priority': priority,
            'tags': tags[:5],  # é™åˆ¶æœ€å¤š5ä¸ªæ ‡ç­¾
            'is_favorited': False
        }

        articles.append(article)

        # ç¤¼è²Œå»¶è¿Ÿ
        time.sleep(random.uniform(1.5, 3))

    print(f"\nå¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(articles)} æ¡ç»“æ„åŒ–æ•°æ®", file=sys.stderr)
    return articles


def save_to_supabase(articles: List[Dict], url: str, key: str, table_name: str = 'school_notices'):
    """
    ä¸Šä¼ æ•°æ®åˆ° Supabase

    Args:
        articles: æ–‡ç« æ•°æ®åˆ—è¡¨
        url: Supabase URL
        key: Supabase API Key
        table_name: ç›®æ ‡è¡¨åï¼ˆé»˜è®¤ school_noticesï¼‰
    """
    if not create_client:
        print("é”™è¯¯: æœªå®‰è£… supabase åŒ…ï¼Œè¯·è¿è¡Œ: pip install supabase", file=sys.stderr)
        return

    print(f"\nè¿æ¥ Supabase æ•°æ®åº“...", file=sys.stderr)

    try:
        supabase: Client = create_client(url, key)

        uploaded_count = 0
        skipped_count = 0

        for article in articles:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸºäº source_url å»é‡ï¼‰
                existing = supabase.table(table_name).select('id').eq('source_url', article['source_url']).execute()

                if existing.data:
                    print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {article['title'][:25]}...", file=sys.stderr)
                    skipped_count += 1
                else:
                    # æ’å…¥æ–°æ•°æ®
                    supabase.table(table_name).insert(article).execute()
                    print(f"âœ… ä¸Šä¼ æˆåŠŸ: {article['title'][:25]}... [ä¼˜å…ˆçº§: {article['priority']}]", file=sys.stderr)
                    uploaded_count += 1

            except Exception as e:
                print(f"âŒ ä¸Šä¼ å¤±è´¥ ({article['title'][:20]}): {e}", file=sys.stderr)

        print(f"\nğŸ“Š ä¸Šä¼ ç»Ÿè®¡: æ–°å¢ {uploaded_count} æ¡, è·³è¿‡ {skipped_count} æ¡", file=sys.stderr)

    except Exception as e:
        print(f"âŒ Supabase è¿æ¥é”™è¯¯: {e}", file=sys.stderr)


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åå—ç†å·¥å¤§å­¦æ•™åŠ¡å¤„é€šçŸ¥çˆ¬è™«')
    parser.add_argument('--pages', type=int, default=2, help='æŠ“å–é¡µæ•°ï¼ˆé»˜è®¤ 2ï¼‰')
    parser.add_argument('--limit', type=int, default=10, help='å¤„ç†é€šçŸ¥æ•°é‡ï¼ˆé»˜è®¤ 10ï¼‰')
    parser.add_argument('--category', type=int, default=0, help='é€šçŸ¥åˆ†ç±» (0=å…¨éƒ¨, 1=é€‰è¯¾, 2=è€ƒè¯•, 3=å®è·µ, 4=äº¤æµ, 5=æ•™å¸ˆ, 6=ä¿¡æ¯)')
    parser.add_argument('--output', default='', help='è¾“å‡º JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--upload', action='store_true', help='ä¸Šä¼ åˆ° Supabase')
    parser.add_argument('--table', default='school_notices', help='Supabase è¡¨åï¼ˆé»˜è®¤ school_noticesï¼‰')

    # Supabase é…ç½®ï¼ˆä¸ GitHub è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    default_url = "https://ovytvktzhuapvictznnr.supabase.co"
    default_key = "sb_secret_3DFQXsH8RqIldbdJAVrC5Q_A_xtu9uh"

    parser.add_argument('--supabase-url', default=default_url, help='Supabase Project URL')
    parser.add_argument('--supabase-key', default=default_key, help='Supabase API Key')

    args = parser.parse_args()

    # Step 1: æŠ“å–é€šçŸ¥åˆ—è¡¨
    notices = fetch_notice_list(max_pages=args.pages, category=args.category)

    if not notices:
        print("âš ï¸  æœªæŠ“å–åˆ°ä»»ä½•é€šçŸ¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç½‘ç«™ç»“æ„æ˜¯å¦å˜åŒ–", file=sys.stderr)
        sys.exit(1)

    print(f"\nâœ… å…±æŠ“å–åˆ° {len(notices)} æ¡é€šçŸ¥", file=sys.stderr)

    # Step 2: å¤„ç†é€šçŸ¥è¯¦æƒ…
    articles = process_notices(notices, limit=args.limit)

    # Step 3: è¾“å‡ºåˆ°æ–‡ä»¶
    if args.output:
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(articles, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {args.output}", file=sys.stderr)
        except IOError as e:
            print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}", file=sys.stderr)
    elif not args.upload:
        # ä¸ä¸Šä¼ ä¸”ä¸ä¿å­˜æ–‡ä»¶æ—¶ï¼Œæ‰“å°åˆ°æ ‡å‡†è¾“å‡º
        print(json.dumps(articles, indent=2, ensure_ascii=False))

    # Step 4: ä¸Šä¼ åˆ° Supabase
    if args.upload:
        url = args.supabase_url or os.environ.get('SUPABASE_URL')
        key = args.supabase_key or os.environ.get('SUPABASE_KEY')

        if url and key:
            save_to_supabase(articles, url, key, args.table)
        else:
            print("âŒ é”™è¯¯: éœ€è¦æä¾› Supabase URL å’Œ Key", file=sys.stderr)
            print("è¯·é€šè¿‡å‚æ•° --supabase-url/--supabase-key æˆ–ç¯å¢ƒå˜é‡æä¾›", file=sys.stderr)


if __name__ == '__main__':
    main()
