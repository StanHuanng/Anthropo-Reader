#!/usr/bin/env python3
"""
GitHub Trending Data Fetching Script
Fetches trending repositories from GitHub and uploads to Supabase
é›†æˆç¡…åŸºæµåŠ¨ AI æ‘˜è¦ç”ŸæˆåŠŸèƒ½
"""

import requests
import json
import os
import sys
import argparse
from datetime import datetime

# Try to import supabase
try:
    from supabase import create_client, Client
except ImportError:
    create_client = None

# Try to import AI summarizer
try:
    from ai_summarizer import generate_summary
except ImportError:
    generate_summary = None

def fetch_trending_repos(language='', limit=20, use_ai=False, api_key=None):
    """
    Fetch GitHub Trending repositories - æ™ºèƒ½ç­›é€‰å‰æ²¿é¡¹ç›®

    ç­›é€‰ç­–ç•¥ï¼š
    1. åªæŠ“å–æœ€è¿‘30å¤©åˆ›å»ºçš„æ–°é¡¹ç›®
    2. è¿‡æ»¤æ‰ awesome/æ•™ç¨‹/é¢è¯• ç­‰æ”¶é›†ç±»é¡¹ç›®
    3. ä¼˜å…ˆå±•ç¤º AI/å·¥å…·/App ç±»é¡¹ç›®

    Args:
        language: Programming language filter
        limit: Number of results
        use_ai: Whether to generate AI summaries
        api_key: SiliconFlow API key for AI summaries
    """
    from datetime import timedelta
    import re

    # ==================== ç­›é€‰é…ç½® ====================

    # é»‘åå•ï¼šè¿‡æ»¤æ”¶é›†ç±»/æ•™ç¨‹ç±»/èµ„æºç±»é¡¹ç›®
    EXCLUDE_PATTERNS = [
        # æ”¶é›†ç±»
        r'^awesome[-_]', r'[-_]awesome$', r'[-_]list$', r'^list[-_]',
        r'resources', r'curated', r'collection',
        # æ•™ç¨‹/å­¦ä¹ ç±»
        r'interview', r'learning', r'^learn[-_]', r'[-_]learn$',
        r'tutorial', r'course', r'guide', r'handbook',
        r'roadmap', r'cheatsheet', r'notes',
        # çº¯ç´ æç±»
        r'^icons?$', r'^fonts?$', r'wallpaper', r'design[-_]resources',
        # å…¶ä»–ä½ä»·å€¼
        r'free[-_]programming', r'coding[-_]interview',
        r'system[-_]design', r'algorithm', r'leetcode',
    ]

    # ä¼˜å…ˆå…³é”®è¯ï¼šAI/å·¥å…·/App ç›¸å…³ï¼ˆæƒé‡ +100ï¼‰
    PRIORITY_KEYWORDS = [
        # AI/LLM å‰æ²¿
        'ai', 'llm', 'gpt', 'claude', 'agent', 'mcp',
        'anthropic', 'openai', 'gemini', 'ollama', 'langchain',
        'rag', 'embedding', 'vector', 'chatbot',
        # å¼€å‘å·¥å…·
        'cursor', 'copilot', 'vscode', 'neovim', 'vim',
        'terminal', 'cli', 'sdk', 'api', 'devtools',
        # å®ç”¨ App/å®¢æˆ·ç«¯
        'app', 'desktop', 'client', 'gui', 'native',
        'macos', 'windows', 'linux', 'cross-platform',
        'tauri', 'electron', 'flutter',
        # æ•ˆç‡å·¥å…·
        'productivity', 'automation', 'workflow', 'utility',
        'tool', 'assistant', 'helper', 'manager',
        # æ–°å…´æŠ€æœ¯
        'rust', 'zig', 'bun', 'deno', 'wasm', 'webassembly',
    ]

    # ==================== è¾…åŠ©å‡½æ•° ====================

    def should_exclude(repo_name: str, description: str) -> bool:
        """æ£€æŸ¥é¡¹ç›®æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
        text = f"{repo_name} {description}".lower()
        for pattern in EXCLUDE_PATTERNS:
            if re.search(pattern, text):
                return True
        return False

    def calculate_priority(repo: dict) -> int:
        """è®¡ç®—é¡¹ç›®ä¼˜å…ˆçº§åˆ†æ•°"""
        name = repo.get('name', '').lower()
        desc = (repo.get('description') or '').lower()
        text = f"{name} {desc}"

        score = 0
        for keyword in PRIORITY_KEYWORDS:
            if keyword in text:
                score += 100

        # æ–°é¡¹ç›®åŠ åˆ†ï¼ˆåˆ›å»ºæ—¶é—´è¶Šè¿‘åˆ†æ•°è¶Šé«˜ï¼‰
        try:
            created = datetime.fromisoformat(repo['created_at'].replace('Z', '+00:00'))
            days_ago = (datetime.now(created.tzinfo) - created).days
            if days_ago <= 7:
                score += 50  # ä¸€å‘¨å†…åˆ›å»º
            elif days_ago <= 14:
                score += 30  # ä¸¤å‘¨å†…åˆ›å»º
        except:
            pass

        return score

    # ==================== æŠ“å–é€»è¾‘ ====================

    url = "https://api.github.com/search/repositories"

    # æŸ¥è¯¢ï¼šæœ€è¿‘ 30 å¤©åˆ›å»ºçš„é¡¹ç›®ï¼Œè‡³å°‘ 100 Stars
    thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
    query = f'created:>{thirty_days_ago} stars:>100'
    if language:
        query += f' language:{language}'

    params = {
        'q': query,
        'sort': 'stars',
        'order': 'desc',
        'per_page': min(limit * 3, 100)  # å¤šæŠ“ä¸€äº›ç”¨äºè¿‡æ»¤
    }

    try:
        print(f"ğŸ” æŸ¥è¯¢æ¡ä»¶: {query}", file=sys.stderr)
        response = requests.get(url, params=params)
        response.raise_for_status()
        repos = response.json().get('items', [])

        print(f"ğŸ“¦ è·å–åˆ° {len(repos)} ä¸ªåŸå§‹é¡¹ç›®", file=sys.stderr)

        # Step 1: è¿‡æ»¤é»‘åå•é¡¹ç›®
        filtered_repos = []
        excluded_count = 0
        for repo in repos:
            name = repo.get('name', '')
            desc = repo.get('description') or ''
            if should_exclude(name, desc):
                excluded_count += 1
                print(f"  âŒ è¿‡æ»¤: {name}", file=sys.stderr)
            else:
                filtered_repos.append(repo)

        print(f"ğŸ§¹ è¿‡æ»¤æ‰ {excluded_count} ä¸ªæ”¶é›†ç±»/æ•™ç¨‹ç±»é¡¹ç›®", file=sys.stderr)

        # Step 2: æŒ‰ä¼˜å…ˆçº§æ’åº
        for repo in filtered_repos:
            repo['_priority'] = calculate_priority(repo)

        filtered_repos.sort(key=lambda x: (x['_priority'], x['stargazers_count']), reverse=True)

        # Step 3: å–å‰ limit ä¸ª
        final_repos = filtered_repos[:limit]

        print(f"âœ… æœ€ç»ˆé€‰å– {len(final_repos)} ä¸ªä¼˜è´¨é¡¹ç›®", file=sys.stderr)

        articles = []
        for i, repo in enumerate(final_repos):
            # Build article content
            content = f"""# {repo['name']}

{repo['description'] or 'No description provided.'}

## Project Info
- **Stars**: {repo['stargazers_count']:,}
- **Language**: {repo['language'] or 'N/A'}
- **Forks**: {repo['forks_count']:,}
- **Open Issues**: {repo['open_issues_count']}
- **Created**: {repo['created_at'][:10]}
- **Last Updated**: {repo['updated_at'][:10]}

## Links
[View Project]({repo['html_url']})

## Author
[{repo['owner']['login']}]({repo['owner']['html_url']})
"""

            article = {
                'title': repo['name'],
                'summary': (repo['description'] or '')[:300],
                'content': content,
                'source': 'github_trending',
                'source_url': repo['html_url'],
                'author': repo['owner']['login'],
                'published_at': repo['created_at'],
                'fetched_at': datetime.now().isoformat(),
                'tags': [repo['language']] if repo['language'] else [],
                'is_favorited': False,
                'ai_summary': None  # Will be filled if use_ai is True
            }

            # Generate AI summary if enabled
            if use_ai and generate_summary:
                print(f"[{i+1}/{len(repos)}] ğŸ¤– ç”Ÿæˆ AI æ‘˜è¦: {repo['name'][:30]}...", file=sys.stderr)
                ai_summary = generate_summary(
                    content=content,
                    content_type='github',
                    api_key=api_key
                )
                if ai_summary:
                    article['ai_summary'] = ai_summary
                    # ç”¨ AI æ‘˜è¦æ›¿æ¢åŸå§‹ contentï¼Œä¿ç•™åŸå§‹é“¾æ¥
                    article['content'] = f"""# {repo['name']}

{ai_summary}

---

## ğŸ“ åŸå§‹é“¾æ¥
[æŸ¥çœ‹ GitHub é¡¹ç›®]({repo['html_url']})

## ğŸ“Š é¡¹ç›®æ•°æ®
- â­ Stars: {repo['stargazers_count']:,}
- ğŸ´ Forks: {repo['forks_count']:,}
- ğŸ’» Language: {repo['language'] or 'N/A'}
- ğŸ‘¤ Author: [{repo['owner']['login']}]({repo['owner']['html_url']})
"""
                # ç¤¼è²Œå»¶è¿Ÿé¿å… API é™æµ
                import time
                import random
                if i < len(repos) - 1:
                    time.sleep(random.uniform(1, 2))

            articles.append(article)

        return articles

    except requests.exceptions.RequestException as e:
        print(f"Error: Failed to fetch data - {e}", file=sys.stderr)
        return []

def save_to_supabase(articles, url, key):
    """
    Upload articles to Supabase
    """
    if not create_client:
        print("Error: 'supabase' package not installed. Run: pip install supabase", file=sys.stderr)
        return

    print(f"Connecting to Supabase...", file=sys.stderr)
    try:
        supabase: Client = create_client(url, key)

        count = 0
        for article in articles:
            try:
                # Check for duplicates based on source_url
                # Note: This requires the key to have SELECT permissions
                existing = supabase.table('articles').select('id').eq('source_url', article['source_url']).execute()

                if existing.data:
                    print(f"Skipping existing: {article['title'][:20]}...", file=sys.stderr)
                else:
                    # Insert new article
                    # Note: This requires the key to have INSERT permissions (Service Role Key recommended)
                    supabase.table('articles').insert(article).execute()
                    print(f"Uploaded: {article['title'][:20]}...", file=sys.stderr)
                    count += 1

            except Exception as e:
                print(f"Error uploading {article['title'][:20]}: {e}", file=sys.stderr)

        print(f"Upload complete. New articles: {count}", file=sys.stderr)

    except Exception as e:
        print(f"Supabase connection error: {e}", file=sys.stderr)

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Fetch GitHub Trending Data')
    parser.add_argument('--language', default='', help='Programming language filter')
    parser.add_argument('--limit', type=int, default=20, help='Number of results')
    parser.add_argument('--output', default='', help='Output file path')
    parser.add_argument('--upload', action='store_true', help='Upload to Supabase')
    parser.add_argument('--ai', action='store_true', help='Generate AI summaries using SiliconFlow')
    parser.add_argument('--ai-key', default='', help='SiliconFlow API Key (or use SILICONFLOW_API_KEY env)')

    # Args for Supabase credentials (optional, can use env vars)
    # Defaulting to provided credentials for ease of use
    default_url = "https://ovytvktzhuapvictznnr.supabase.co"
    default_key = "sb_secret_3DFQXsH8RqIldbdJAVrC5Q_A_xtu9uh"

    parser.add_argument('--supabase-url', default=default_url, help='Supabase Project URL')
    parser.add_argument('--supabase-key', default=default_key, help='Supabase API Key')

    args = parser.parse_args()

    # Check AI requirements
    if args.ai:
        if not generate_summary:
            print("âš ï¸ AI æ‘˜è¦åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿ ai_summarizer.py åœ¨åŒä¸€ç›®å½•", file=sys.stderr)
            args.ai = False
        else:
            ai_key = args.ai_key or os.environ.get('SILICONFLOW_API_KEY')
            if not ai_key:
                print("âŒ é”™è¯¯: ä½¿ç”¨ --ai éœ€è¦æä¾›ç¡…åŸºæµåŠ¨ API Key", file=sys.stderr)
                print("   ä½¿ç”¨ --ai-key YOUR_KEY æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ SILICONFLOW_API_KEY", file=sys.stderr)
                sys.exit(1)
            print("ğŸ¤– AI æ‘˜è¦åŠŸèƒ½å·²å¯ç”¨ (ç¡…åŸºæµåŠ¨)", file=sys.stderr)

    print(f"Fetching GitHub Trending data...", file=sys.stderr)

    # Get AI key
    ai_key = args.ai_key or os.environ.get('SILICONFLOW_API_KEY') if args.ai else None

    articles = fetch_trending_repos(
        language=args.language,
        limit=args.limit,
        use_ai=args.ai,
        api_key=ai_key
    )

    print(f"Successfully fetched {len(articles)} articles", file=sys.stderr)

    # Handle Output
    if args.output:
        try:
            output_data = json.dumps(articles, indent=2, ensure_ascii=False)
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output_data)
            print(f"Saved to: {args.output}", file=sys.stderr)
        except IOError as e:
             print(f"Error saving file: {e}", file=sys.stderr)
    elif not args.upload:
        print(json.dumps(articles, indent=2, ensure_ascii=False))

    # Handle Upload
    if args.upload:
        # Prioritize args, then env vars
        url = args.supabase_url or os.environ.get('SUPABASE_URL')
        key = args.supabase_key or os.environ.get('SUPABASE_KEY') # Prefer SERVICE_ROLE_KEY for writing

        if url and key:
            save_to_supabase(articles, url, key)
        else:
            print("Error: Supabase URL and Key required for upload.", file=sys.stderr)
            print("Provide via arguments --supabase-url/--supabase-key or environment variables.", file=sys.stderr)

if __name__ == '__main__':
    main()
