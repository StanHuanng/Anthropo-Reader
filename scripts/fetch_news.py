#!/usr/bin/env python3
"""
å¤šæºæ–°é—»èšåˆçˆ¬è™« (Anthropo-Reader)
æ”¯æŒ: BBCä¸­æ–‡ã€çº½çº¦æ—¶æŠ¥ä¸­æ–‡ã€åå°”è¡—æ—¥æŠ¥ä¸­æ–‡ã€ç»æµå­¦äºº
ç‰¹ç‚¹: ä¸“æ³¨äºé«˜è´¨é‡æ·±åº¦æŠ¥é“ï¼Œæ”¯æŒ AI æ‘˜è¦å’Œä¼˜å…ˆçº§æ ‡è®°
"""

import requests
import feedparser
import json
import os
import sys
import argparse
import time
import re
from datetime import datetime
from typing import List, Dict, Optional
import opencc
import html2text
from bs4 import BeautifulSoup

# Try to import supabase
try:
    from supabase import create_client, Client
except ImportError:
    create_client = None

# åˆå§‹åŒ–è½¬æ¢å™¨
cc = opencc.OpenCC('t2s')  # ç¹ä½“è½¬ç®€ä½“

# ==================== é…ç½®åŒº ====================

NEWS_SOURCES = {
    # --- å›½é™…æ–°é—» (RSS) ---
    'bbc_chinese': {
        'name': 'BBCä¸­æ–‡',
        'category': 'international',
        'type': 'rss',
        'url': 'https://feeds.bbci.co.uk/zhongwen/simp/rss.xml',
        'source_id': 'news_international',
    },
    'nytimes_chinese': {
        'name': 'çº½çº¦æ—¶æŠ¥ä¸­æ–‡',
        'category': 'international',
        'type': 'rss',
        'url': 'https://cn.nytimes.com/rss/',
        'source_id': 'news_international',
    },
    'wsj_chinese': {
        'name': 'åå°”è¡—æ—¥æŠ¥ä¸­æ–‡',
        'category': 'international',
        'type': 'rss',
        'url': 'https://cn.wsj.com/zh-hans/rss',
        'source_id': 'news_international',
    },
    'economist': {
        'name': 'The Economist',
        'category': 'international',
        'type': 'rss',
        'url': 'https://www.economist.com/the-world-this-week/rss.xml',
        'source_id': 'news_international',
    },
}

# ä¼˜å…ˆçº§å…³é”®è¯
HIGH_PRIORITY_KEYWORDS = [
    'æ”¿æ²»', 'ç»æµ', 'æ”¿ç­–', 'GDP', 'è´¸æ˜“', 'é€‰ä¸¾',
    'AI', 'äººå·¥æ™ºèƒ½', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'GPT', 'LLM',
    'Apple', 'Google', 'Microsoft', 'OpenAI', 'Huawei',
    'è£å‘˜', 'èèµ„', 'ä¸Šå¸‚', 'é‡å¤§', 'çªå‘', 'æ·±åº¦', 'è°ƒæŸ¥'
]

# ==================== æ ¸å¿ƒåŠŸèƒ½ ====================

def clean_html(html_content: str) -> str:
    """ç®€å•çš„ HTML æ¸…ç†"""
    if not html_content:
        return ""
    text = re.sub(r'<(script|style).*?>.*?</\1>', '', html_content, flags=re.DOTALL)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def convert_to_simplified(text: str) -> str:
    """ç¹ä½“è½¬ç®€ä½“"""
    if not text: return ""
    return cc.convert(text)

def calculate_priority(title: str, category: str) -> str:
    """è®¡ç®—æ–‡ç« ä¼˜å…ˆçº§"""
    # å›½é™…æ·±åº¦æŠ¥é“é»˜è®¤è¾ƒé«˜
    if category in ['international']:
        base_score = 1
    else:
        base_score = 0

    # å…³é”®è¯åŒ¹é…
    for kw in HIGH_PRIORITY_KEYWORDS:
        if kw.lower() in title.lower():
            return 'high'

    return 'high' if base_score > 0 else 'low'

def fetch_rss_news(source_key: str, limit: int = 10) -> List[Dict]:
    """æŠ“å– RSS æ–°é—»æº"""
    config = NEWS_SOURCES[source_key]
    print(f"ğŸ“¡ æ­£åœ¨æŠ“å– RSS: {config['name']}...", file=sys.stderr)

    try:
        # æ·»åŠ  User-Agent
        feed = feedparser.parse(config['url'], agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        articles = []

        for entry in feed.entries[:limit]:
            content = ""
            if 'content' in entry:
                content = entry.content[0].value
            elif 'summary' in entry:
                content = entry.summary

            clean_content = clean_html(content)
            if not clean_content:
                clean_content = entry.title

            # ç¹ç®€è½¬æ¢ (å¯¹è‹±æ–‡å†…å®¹æ— å½±å“)
            title = convert_to_simplified(entry.title)
            clean_content = convert_to_simplified(clean_content)

            # è®¡ç®—ä¼˜å…ˆçº§
            priority = calculate_priority(title, config['category'])

            published_at = datetime.now().isoformat()
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_at = datetime(*entry.published_parsed[:6]).isoformat()

            articles.append({
                'title': title,
                'summary': clean_content[:200] + '...',
                'content': f"# {title}\n\n> æ¥æº: {config['name']} | {published_at[:10]}\n\n{clean_content}\n\n[æŸ¥çœ‹åŸæ–‡]({entry.link})",
                'source': config['source_id'],
                'source_url': entry.link,
                'author': config['name'],
                'category': config['category'],
                'priority': priority,
                'published_at': published_at,
                'fetched_at': datetime.now().isoformat(),
                'tags': [config['name'], config['category']],
            })

        print(f"âœ… {config['name']}: è·å– {len(articles)} æ¡", file=sys.stderr)
        return articles
    except Exception as e:
        print(f"âŒ {config['name']} æŠ“å–å¤±è´¥: {e}", file=sys.stderr)
        return []

def process_with_ai(articles: List[Dict], api_key: str):
    """ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦"""
    try:
        from ai_summarizer import generate_summary

        print(f"\nğŸ¤– å¼€å§‹ AI æ‘˜è¦ç”Ÿæˆ (å…± {len(articles)} æ¡)...", file=sys.stderr)

        count = 0
        for i, article in enumerate(articles):
            if len(article['content']) < 100: continue
            if count > 0: time.sleep(1.5)

            # ä¸å†å¼ºåˆ¶ç¿»è¯‘ï¼Œç»Ÿä¸€ä½¿ç”¨ news ç±»å‹ç”Ÿæˆæ‘˜è¦
            print(f"[{i+1}/{len(articles)}] ç”Ÿæˆæ‘˜è¦: {article['title'][:20]}...", file=sys.stderr)

            ai_summary = generate_summary(article['content'], 'news', api_key)

            if ai_summary:
                article['ai_summary'] = ai_summary
                count += 1
            else:
                print(f"  âš ï¸ ç”Ÿæˆå¤±è´¥", file=sys.stderr)

    except ImportError:
        print("âŒ æœªæ‰¾åˆ° ai_summarizer æ¨¡å—ï¼Œè·³è¿‡ AI æ‘˜è¦", file=sys.stderr)
    except Exception as e:
        print(f"âŒ AI å¤„ç†å‡ºé”™: {e}", file=sys.stderr)

def save_to_supabase(articles: List[Dict], url: str, key: str):
    """ä¸Šä¼ æ•°æ®åˆ° Supabase"""
    if not create_client:
        print("âŒ æœªå®‰è£… supabase åº“", file=sys.stderr)
        return

    print(f"\nğŸ’¾ è¿æ¥ Supabase...", file=sys.stderr)
    try:
        supabase: Client = create_client(url, key)
        success = 0
        skipped = 0

        for article in articles:
            try:
                # æ£€æŸ¥é‡å¤
                existing = supabase.table('news').select('id').eq('source_url', article['source_url']).execute()
                if existing.data:
                    skipped += 1
                    continue

                supabase.table('news').insert(article).execute()
                success += 1
                print(f"  âœ… ä¸Šä¼ : {article['title'][:20]}... [{article['priority']}]", file=sys.stderr)
            except Exception as e:
                print(f"  âŒ ä¸Šä¼ å¤±è´¥: {e}", file=sys.stderr)

        print(f"ğŸ“Š å®Œæˆ: æ–°å¢ {success}, è·³è¿‡ {skipped}", file=sys.stderr)

    except Exception as e:
        print(f"âŒ Supabase è¿æ¥å¤±è´¥: {e}", file=sys.stderr)

def main():
    parser = argparse.ArgumentParser(description='å¤šæºæ–°é—»èšåˆçˆ¬è™«')
    parser.add_argument('--upload', action='store_true', help='ä¸Šä¼ åˆ° Supabase')
    parser.add_argument('--ai', action='store_true', help='å¯ç”¨ AI æ‘˜è¦')
    parser.add_argument('--limit', type=int, default=10, help='æ¯ä¸ªæºçš„é™åˆ¶æ•°é‡')
    parser.add_argument('--supabase-url', default=os.environ.get('SUPABASE_URL'), help='Supabase URL')
    parser.add_argument('--supabase-key', default=os.environ.get('SUPABASE_KEY'), help='Supabase Key')

    args = parser.parse_args()
    api_key = os.environ.get('SILICONFLOW_API_KEY')

    all_news = []

    # æŠ“å–å„é«˜è´¨é‡æº
    all_news.extend(fetch_rss_news('bbc_chinese', limit=args.limit))
    all_news.extend(fetch_rss_news('nytimes_chinese', limit=args.limit))
    all_news.extend(fetch_rss_news('wsj_chinese', limit=args.limit))
    all_news.extend(fetch_rss_news('economist', limit=args.limit))

    print(f"\nğŸ“¦ å…±æŠ“å–åˆ° {len(all_news)} æ¡æ–°é—»", file=sys.stderr)

    # AI å¤„ç†
    if args.ai and api_key:
        process_with_ai(all_news, api_key)

    # ä¸Šä¼ 
    if args.upload:
        if args.supabase_url and args.supabase_key:
            save_to_supabase(all_news, args.supabase_url, args.supabase_key)
        else:
            print("âŒ ç¼ºå°‘ Supabase é…ç½®ï¼Œæ— æ³•ä¸Šä¼ ", file=sys.stderr)
    else:
        # æœ¬åœ°æµ‹è¯•
        print(json.dumps(all_news[:2], indent=2, ensure_ascii=False))

if __name__ == '__main__':
    main()
