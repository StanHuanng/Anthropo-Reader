#!/usr/bin/env python3
"""
å¤šæºæ–°é—»èšåˆçˆ¬è™« (Anthropo-Reader)
æ”¯æŒ: å¾®åšçƒ­æœã€çŸ¥ä¹çƒ­æ¦œã€BBCä¸­æ–‡ã€36Krã€å°‘æ•°æ´¾ã€TechCrunchã€æå®¢å…¬å›­
å¢å¼ºåŠŸèƒ½: ç®€ç¹è½¬æ¢ã€AI ç¿»è¯‘ã€ä¼˜å…ˆçº§æ ‡è®°
"""

import requests
import feedparser
import json
import os
import sys
import argparse
import time
import re
from datetime import datetime
from typing import List, Dict, Optional
import opencc
import html2text
from bs4 import BeautifulSoup

# Try to import supabase
try:
    from supabase import create_client, Client
except ImportError:
    create_client = None

# åˆå§‹åŒ–è½¬æ¢å™¨
cc = opencc.OpenCC('t2s')  # ç¹ä½“è½¬ç®€ä½“

# ==================== é…ç½®åŒº ====================

NEWS_SOURCES = {
    # --- å›½å†…çƒ­ç‚¹ (API) ---
    'weibo': {
        'name': 'å¾®åšçƒ­æœ',
        'category': 'domestic',
        'type': 'api',
        'url': 'https://weibo.com/ajax/side/hotSearch',
        'source_id': 'news_domestic',
    },

    # --- å›½é™…æ–°é—» (RSS) ---
    'bbc_chinese': {
        'name': 'BBCä¸­æ–‡',
        'category': 'international',
        'type': 'rss',
        'url': 'https://feeds.bbci.co.uk/zhongwen/simp/rss.xml',
        'source_id': 'news_international',
    },
    'nytimes_chinese': {
        'name': 'çº½çº¦æ—¶æŠ¥ä¸­æ–‡',
        'category': 'international',
        'type': 'rss',
        'url': 'https://cn.nytimes.com/rss/',
        'source_id': 'news_international',
    },

    # --- ç§‘æŠ€äº‹ä»¶ (RSS) ---
    '36kr': {
        'name': '36æ°ª',
        'category': 'tech',
        'type': 'rss',
        'url': 'https://36kr.com/feed',
        'source_id': 'news_tech',
    },
    'geekpark': {
        'name': 'æå®¢å…¬å›­',
        'category': 'tech',
        'type': 'rss',
        'url': 'https://www.geekpark.net/rss',
        'source_id': 'news_tech',
    },
    'techcrunch': {
        'name': 'TechCrunch',
        'category': 'tech',
        'type': 'rss',
        'url': 'https://techcrunch.com/feed/',
        'source_id': 'news_tech',
    },
    'sspai': {
        'name': 'å°‘æ•°æ´¾',
        'category': 'tech',
        'type': 'rss',
        'url': 'https://sspai.com/feed',
        'source_id': 'news_tech',
    },
}

# ä¼˜å…ˆçº§å…³é”®è¯
HIGH_PRIORITY_KEYWORDS = [
    'æ”¿æ²»', 'ç»æµ', 'æ”¿ç­–', 'GDP', 'è´¸æ˜“', 'é€‰ä¸¾',
    'AI', 'äººå·¥æ™ºèƒ½', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'GPT', 'LLM',
    'Apple', 'Google', 'Microsoft', 'OpenAI', 'Huawei',
    'è£å‘˜', 'èèµ„', 'ä¸Šå¸‚', 'é‡å¤§', 'çªå‘'
]

# ==================== æ ¸å¿ƒåŠŸèƒ½ ====================

def clean_html(html_content: str) -> str:
    """ç®€å•çš„ HTML æ¸…ç†"""
    if not html_content:
        return ""
    text = re.sub(r'<(script|style).*?>.*?</\1>', '', html_content, flags=re.DOTALL)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def convert_to_simplified(text: str) -> str:
    """ç¹ä½“è½¬ç®€ä½“"""
    if not text: return ""
    return cc.convert(text)

def is_english(text: str) -> bool:
    """ç®€å•çš„è‹±æ–‡æ£€æµ‹"""
    if not text: return False
    eng_chars = sum(1 for c in text if 'a' <= c.lower() <= 'z')
    return eng_chars / len(text) > 0.5 if len(text) > 0 else False

def calculate_priority(title: str, category: str) -> str:
    """è®¡ç®—æ–‡ç« ä¼˜å…ˆçº§"""
    # 1. ç§‘æŠ€å’Œå›½é™…ç±»é»˜è®¤è¾ƒé«˜
    if category in ['tech', 'international']:
        base_score = 1
    else:
        base_score = 0

    # 2. å…³é”®è¯åŒ¹é…
    for kw in HIGH_PRIORITY_KEYWORDS:
        if kw.lower() in title.lower():
            return 'high'

    return 'high' if base_score > 0 else 'low'

def fetch_rss_news(source_key: str, limit: int = 10) -> List[Dict]:
    """æŠ“å– RSS æ–°é—»æº"""
    config = NEWS_SOURCES[source_key]
    print(f"ğŸ“¡ æ­£åœ¨æŠ“å– RSS: {config['name']}...", file=sys.stderr)

    try:
        # æ·»åŠ  User-Agent ä»¥é€šè¿‡ç®€å•çš„åçˆ¬è™«æ£€æŸ¥
        feed = feedparser.parse(config['url'], agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        articles = []

        for entry in feed.entries[:limit]:
            content = ""
            if 'content' in entry:
                content = entry.content[0].value
            elif 'summary' in entry:
                content = entry.summary

            clean_content = clean_html(content)
            if not clean_content:
                clean_content = entry.title

            # ç¹ç®€è½¬æ¢
            title = convert_to_simplified(entry.title)
            clean_content = convert_to_simplified(clean_content)

            # è®¡ç®—ä¼˜å…ˆçº§
            priority = calculate_priority(title, config['category'])

            published_at = datetime.now().isoformat()
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_at = datetime(*entry.published_parsed[:6]).isoformat()

            articles.append({
                'title': title,
                'summary': clean_content[:200] + '...',
                'content': f"# {title}\n\n> æ¥æº: {config['name']} | {published_at[:10]}\n\n{clean_content}\n\n[æŸ¥çœ‹åŸæ–‡]({entry.link})",
                'source': config['source_id'],
                'source_url': entry.link,
                'author': config['name'],
                'category': config['category'],
                'priority': priority,
                'published_at': published_at,
                'fetched_at': datetime.now().isoformat(),
                'tags': [config['name'], config['category']],
            })

        print(f"âœ… {config['name']}: è·å– {len(articles)} æ¡", file=sys.stderr)
        return articles
    except Exception as e:
        print(f"âŒ {config['name']} æŠ“å–å¤±è´¥: {e}", file=sys.stderr)
        return []

def fetch_weibo_hot(limit: int = 15) -> List[Dict]:
    """æŠ“å–å¾®åšçƒ­æœ"""
    config = NEWS_SOURCES['weibo']
    print(f"ğŸ“¡ æ­£åœ¨æŠ“å– API: {config['name']}...", file=sys.stderr)

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Cookie': 'SUB=_2AkMVWDUjf8NxqwJRmP0SzGvhZYt2yw_EieKjbJDZJRMxHRl-yT9jqhAbtRB6PfaN_xT-yL9-yL9-yL9-yL9-'
        }
        resp = requests.get(config['url'], headers=headers, timeout=10)
        data = resp.json()

        articles = []
        items = data.get('data', {}).get('realtime', [])

        for item in items[:limit]:
            if 'word' not in item: continue

            title = item.get('word', '')
            note = item.get('note', '') or title
            num = item.get('num', 0)
            url = f"https://s.weibo.com/weibo?q={title}"

            # çƒ­æœå‰3åæˆ–å«æœ‰å…³é”®è¯ä¸ºé«˜ä¼˜å…ˆçº§
            priority = 'high' if (len(articles) < 3 or calculate_priority(title, 'domestic') == 'high') else 'low'

            articles.append({
                'title': title,
                'summary': f"çƒ­åº¦: {num} | {note}",
                'content': f"# {title}\n\n> æ¥æº: å¾®åšçƒ­æœ\n\n**å½“å‰çƒ­åº¦**: {num}\n\n{note}\n\n[æŸ¥çœ‹è®¨è®º]({url})",
                'source': config['source_id'],
                'source_url': url,
                'author': 'å¾®åšçƒ­æœ',
                'category': config['category'],
                'priority': priority,
                'published_at': datetime.now().isoformat(),
                'fetched_at': datetime.now().isoformat(),
                'tags': ['å¾®åš', 'çƒ­æœ', 'domestic'],
            })

        print(f"âœ… {config['name']}: è·å– {len(articles)} æ¡", file=sys.stderr)
        return articles
    except Exception as e:
        print(f"âŒ {config['name']} æŠ“å–å¤±è´¥: {e}", file=sys.stderr)
        return []

def process_with_ai(articles: List[Dict], api_key: str):
    """ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦å’Œç¿»è¯‘"""
    try:
        from ai_summarizer import generate_summary

        print(f"\nğŸ¤– å¼€å§‹ AI æ‘˜è¦ç”Ÿæˆä¸ç¿»è¯‘ (å…± {len(articles)} æ¡)...", file=sys.stderr)

        count = 0
        for i, article in enumerate(articles):
            if len(article['content']) < 100: continue
            if count > 0: time.sleep(1.5)

            # ä½¿ç”¨ is_english_content é¿å…ä¸å‡½æ•°åå†²çª
            is_english_content = article.get('is_english', False) or is_english(article['title'])
            action = "ç¿»è¯‘ä¸æ‘˜è¦" if is_english_content else "ç”Ÿæˆæ‘˜è¦"

            print(f"[{i+1}/{len(articles)}] {action}: {article['title'][:20]}...", file=sys.stderr)

            content_type = 'news_en' if is_english_content else 'news'
            ai_summary = generate_summary(article['content'], content_type, api_key)

            if ai_summary:
                article['ai_summary'] = ai_summary
                count += 1
            else:
                print(f"  âš ï¸ ç”Ÿæˆå¤±è´¥", file=sys.stderr)

    except ImportError:
        print("âŒ æœªæ‰¾åˆ° ai_summarizer æ¨¡å—ï¼Œè·³è¿‡ AI æ‘˜è¦", file=sys.stderr)
    except Exception as e:
        print(f"âŒ AI å¤„ç†å‡ºé”™: {e}", file=sys.stderr)

def save_to_supabase(articles: List[Dict], url: str, key: str):
    """ä¸Šä¼ æ•°æ®åˆ° Supabase"""
    if not create_client:
        print("âŒ æœªå®‰è£… supabase åº“", file=sys.stderr)
        return

    print(f"\nğŸ’¾ è¿æ¥ Supabase...", file=sys.stderr)
    try:
        supabase: Client = create_client(url, key)
        success = 0
        skipped = 0

        for article in articles:
            try:
                # æ£€æŸ¥é‡å¤
                existing = supabase.table('news').select('id').eq('source_url', article['source_url']).execute()
                if existing.data:
                    skipped += 1
                    continue

                supabase.table('news').insert(article).execute()
                success += 1
                print(f"  âœ… ä¸Šä¼ : {article['title'][:20]}... [{article['priority']}]", file=sys.stderr)
            except Exception as e:
                print(f"  âŒ ä¸Šä¼ å¤±è´¥: {e}", file=sys.stderr)

        print(f"ğŸ“Š å®Œæˆ: æ–°å¢ {success}, è·³è¿‡ {skipped}", file=sys.stderr)

    except Exception as e:
        print(f"âŒ Supabase è¿æ¥å¤±è´¥: {e}", file=sys.stderr)

def main():
    parser = argparse.ArgumentParser(description='å¤šæºæ–°é—»èšåˆçˆ¬è™«')
    parser.add_argument('--upload', action='store_true', help='ä¸Šä¼ åˆ° Supabase')
    parser.add_argument('--ai', action='store_true', help='å¯ç”¨ AI æ‘˜è¦')
    parser.add_argument('--limit', type=int, default=10, help='æ¯ä¸ªæºçš„é™åˆ¶æ•°é‡')
    parser.add_argument('--supabase-url', default=os.environ.get('SUPABASE_URL'), help='Supabase URL')
    parser.add_argument('--supabase-key', default=os.environ.get('SUPABASE_KEY'), help='Supabase Key')

    args = parser.parse_args()
    api_key = os.environ.get('SILICONFLOW_API_KEY')

    all_news = []

    # 1. æŠ“å–å„æº
    # å›½å†…
    all_news.extend(fetch_weibo_hot(limit=args.limit))

    # å›½é™…
    all_news.extend(fetch_rss_news('bbc_chinese', limit=args.limit))
    all_news.extend(fetch_rss_news('nytimes_chinese', limit=args.limit))

    # ç§‘æŠ€ (æ–°æº)
    all_news.extend(fetch_rss_news('36kr', limit=args.limit))
    all_news.extend(fetch_rss_news('geekpark', limit=args.limit))
    all_news.extend(fetch_rss_news('techcrunch', limit=args.limit))
    all_news.extend(fetch_rss_news('sspai', limit=args.limit))

    print(f"\nğŸ“¦ å…±æŠ“å–åˆ° {len(all_news)} æ¡æ–°é—»", file=sys.stderr)

    # 2. AI å¤„ç†
    if args.ai and api_key:
        process_with_ai(all_news, api_key)

    # 3. ä¸Šä¼ 
    if args.upload:
        if args.supabase_url and args.supabase_key:
            save_to_supabase(all_news, args.supabase_url, args.supabase_key)
        else:
            print("âŒ ç¼ºå°‘ Supabase é…ç½®ï¼Œæ— æ³•ä¸Šä¼ ", file=sys.stderr)
    else:
        # æœ¬åœ°æµ‹è¯•
        print(json.dumps(all_news[:2], indent=2, ensure_ascii=False))

if __name__ == '__main__':
    main()
